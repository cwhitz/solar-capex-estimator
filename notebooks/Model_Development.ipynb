{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b44a17f8",
   "metadata": {},
   "source": [
    "# Model Development - Solar CAPEX Estimator\n",
    "\n",
    "This notebook develops a machine learning model to predict total installed costs (CAPEX) for commercial solar installations using the LBNL Tracking the Sun dataset. It has five main sections to guide the process from data loading to model evaluation, which will eventually be used in a SolarCapexEstimator class. The five sections are:\n",
    "\n",
    "1) **DataLoader**, responsible for loading Tracking the Sun data from CSV files and filtering it to rows relevant to our use case (commercial solar installations in the US).\n",
    "\n",
    "2) **DataCleaner**, responsible for cleaning the data by removing rows with missing or invalid values in the target column (total installed price). It also drops columns where most values are missing or fills in missing values with appropriate strategies.\n",
    "\n",
    "3) **FeatureEngineer**, responsible for creating new features from the existing data that may help the model learn better, but are still readable and interpretable by users.\n",
    "\n",
    "4) **ModelTrainer**, responsible for training a machine learning model (e.g. linear regression, random forest, or gradient boosting) on the cleaned and feature-engineered data.\n",
    "\n",
    "5) **ModelEvaluator**, responsible for evaluating the trained model's performance using appropriate metrics and validation techniques.\n",
    "\n",
    "Using composition (separate classes coordinated by a higher-level estimator) keeps each step—data loading, cleaning, feature engineering, training, and evaluation—focused on a single responsibility. This improves modularity, testability, and reuse: each component can be developed, swapped, or improved independently without affecting the rest of the pipeline.\n",
    "\n",
    "## Setup and Imports\n",
    "\n",
    "Import necessary libraries for data manipulation, visualization, and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e740d614",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from plotly import graph_objects as go\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3d544c",
   "metadata": {},
   "source": [
    "## 1. DataLoader - Loading TTS Data\n",
    "\n",
    "We'll start by using our **`DataLoader`** class to load the Tracking the Sun dataset. This class handles the messy details of reading CSV files and filtering them to just the records we care about.\n",
    "\n",
    "The `DataLoader` class provides:\n",
    "- Automatic handling of multiple CSV files in a directory\n",
    "- Date parsing for installation dates\n",
    "- Year-based filtering (we want 2019-2023)\n",
    "- Customer segment filtering (commercial and non-residential only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b6a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader:\n",
    "    \"\"\"\n",
    "    Data loader for LBNL Tracking the Sun dataset.\n",
    "\n",
    "    This class handles TTS-specific data loading, cleaning, and filtering\n",
    "    operations independent of the modeling pipeline.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tts_data_directory : str\n",
    "        Path to the directory containing the raw TTS data files.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    raw_data_directory: Path\n",
    "        Directory containing the raw TTS data files.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tts_data_directory: str):\n",
    "        self.tts_data_directory = Path(tts_data_directory)\n",
    "        self.df = None\n",
    "\n",
    "        self.valid_customer_segments = ['COM', 'RES_MF', 'RES_SF', 'RES', 'AGRICULTURAL',\n",
    "       'OTHER TAX-EXEMPT', 'GOV', 'SCHOOL', 'NON-RES', 'NON-PROFIT']\n",
    "\n",
    "    def _filter_by_years(self, df, year_min=None, year_max=None):\n",
    "        \"\"\"\n",
    "        Filter data to specific installation years.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df : pd.DataFrame\n",
    "            Dataframe to filter.\n",
    "        year_min : int, optional\n",
    "            Minimum installation year to include. If None, includes all years.\n",
    "        year_max : int, optional\n",
    "            Maximum installation year to include. If None, includes all years.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Filtered dataframe.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If dataframe has not been loaded.\n",
    "        \"\"\"\n",
    "        if df is None:\n",
    "            raise ValueError(\"Data not loaded. Call load_raw() first.\")\n",
    "\n",
    "        if year_min is not None:\n",
    "            df = df[df.installation_date.dt.year >= year_min]\n",
    "        if year_max is not None:\n",
    "            df = df[df.installation_date.dt.year <= year_max]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def _filter_by_customer_segment(self, df, segments):\n",
    "        \"\"\"\n",
    "        Filter data to specific customer segments.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        segments : list of str\n",
    "            Customer segments to include (e.g., ['COM', 'NON-RES']).\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Filtered dataframe.\n",
    "        segments : list of str\n",
    "            Customer segments to filter to. If None, includes all segments.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If dataframe has not been loaded.\n",
    "        \"\"\"\n",
    "        if df is None:\n",
    "            raise ValueError(\"Data not loaded. Call load_raw() first.\")\n",
    "\n",
    "        df = df[df['customer_segment'].isin(segments)]\n",
    "\n",
    "        return df\n",
    "    \n",
    "    def _validate_filters(self, year_min, year_max, customer_segments):\n",
    "        \"\"\"\n",
    "        Validate filter parameters.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        year_min : int, optional\n",
    "            Minimum installation year to include. If None, includes all years.\n",
    "        year_max : int, optional\n",
    "            Maximum installation year to include. If None, includes all years.\n",
    "        customer_segments : list of str, optional\n",
    "            Customer segments to include (e.g., ['COM', 'NON-RES']).\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If year_min is greater than year_max or if customer_segments is not a list of strings.\n",
    "        \"\"\"\n",
    "        if year_min is not None and year_max is not None and year_min > year_max:\n",
    "            raise ValueError(\"year_min cannot be greater than year_max.\")\n",
    "        \n",
    "        if customer_segments is not None:\n",
    "            if not isinstance(customer_segments, list) or not all(isinstance(seg, str) for seg in customer_segments):\n",
    "                raise ValueError(\"customer_segments must be a list of strings.\")\n",
    "            if not set(customer_segments).issubset(set(self.valid_customer_segments)):\n",
    "                raise ValueError(f\"customer_segments must be a subset of {self.valid_customer_segments}.\")\n",
    "            \n",
    "\n",
    "    def load(\n",
    "        self,\n",
    "        year_min: Optional[int] = None,\n",
    "        year_max: Optional[int] = None,\n",
    "        customer_segments: Optional[List[str]] = None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Load and filter TTS data with common preprocessing steps.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        year_min : int, optional\n",
    "            Minimum year to filter to. If None, includes all years.\n",
    "        year_max : int, optional\n",
    "            Maximum year to filter to. If None, includes all years.\n",
    "        customer_segments : list of str, optional\n",
    "            Customer segments to filter to. If None, includes all segments.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Filtered and cleaned dataframe.\n",
    "        \"\"\"\n",
    "\n",
    "        csvs = list(self.tts_data_directory.glob('*.csv'))\n",
    "\n",
    "        self._validate_filters(year_min, year_max, customer_segments)\n",
    "\n",
    "        if csvs:\n",
    "            self.df = pd.DataFrame()\n",
    "            for csv in csvs:\n",
    "                csv_df = pd.read_csv(csv, parse_dates=['installation_date'])\n",
    "\n",
    "                if year_min is not None or year_max is not None:\n",
    "                    csv_df = self._filter_by_years(csv_df, year_min, year_max)\n",
    "\n",
    "                if customer_segments is not None:\n",
    "                    csv_df = self._filter_by_customer_segment(csv_df, customer_segments)\n",
    "\n",
    "                self.df = pd.concat([self.df, csv_df], ignore_index=True)\n",
    "                print(f\"Loaded {len(csv_df)} rows from {csv.name}\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"No CSV files found in directory {self.tts_data_directory}\")\n",
    "\n",
    "\n",
    "    def get_data(self):\n",
    "        \"\"\"\n",
    "        Get the current dataframe.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        pd.DataFrame\n",
    "            Current dataframe.\n",
    "\n",
    "        Raises\n",
    "        ------\n",
    "        ValueError\n",
    "            If dataframe has not been loaded.\n",
    "        \"\"\"\n",
    "        if self.df is None:\n",
    "            raise ValueError(\"Data not loaded. Call load_raw() or load() first.\")\n",
    "\n",
    "        return self.df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "skts03j9hb",
   "metadata": {},
   "source": [
    "### Instantiate and Load Data\n",
    "\n",
    "Now we'll create a `DataLoader` instance pointing to our raw data directory and use it to load commercial solar installations from 2019-2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc7f85cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zj/gmwqw_hj5bj1f90myf__wr_c0000gn/T/ipykernel_38577/173871540.py:146: DtypeWarning: Columns (1,2,3,11,15,16,18,28,29,31,32,34,35,38,39,40,53,54,56,57,59,60,74,75,79,80) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  csv_df = pd.read_csv(csv, parse_dates=['installation_date'])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 22118 rows from TTS_LBNL_public_file_29-Sep-2025_all.csv\n"
     ]
    }
   ],
   "source": [
    "tts_dataloader = DataLoader(tts_data_directory='../data/raw')\n",
    "\n",
    "tts_dataloader.load(year_min=2019, year_max=2023, customer_segments=['COM', 'NON-RES'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e94ec68",
   "metadata": {},
   "source": [
    "### What We Loaded\n",
    "\n",
    "We successfully loaded 22,118 rows of commercial and non-residential solar installation data from 2019-2023. Each row represents one solar installation project with details about system size, location, components, and cost."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
